{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3016e-3534-47cc-92fd-7b805e565827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# List of user agents for rotation\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    # Add more user agents if necessary\n",
    "]\n",
    "\n",
    "# List of proxies for rotation\n",
    "proxies = [\n",
    "    'http://154.0.12.163:80',\n",
    "    'http://34.175.101.255:80',\n",
    "    'http://173.255.119.18:80',\n",
    "    'http://197.255.125.12:80',\n",
    "    'http://196.20.125.129:8083',\n",
    "    # Add more proxies if necessary\n",
    "]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def get_random_proxy():\n",
    "    return {'http': random.choice(proxies)}\n",
    "\n",
    "def log_request_info(search_query, attempt, proxy, error=None):\n",
    "    with open(\"scraping_log.txt\", \"a\") as log_file:\n",
    "        log_file.write(f\"Attempt {attempt} for {search_query} using {proxy}\\n\")\n",
    "        if error:\n",
    "            log_file.write(f\"Error: {error}\\n\")\n",
    "\n",
    "def search_followers(search_query, retries=3):\n",
    "    \"\"\"\n",
    "    Searches for the follower count of the given search query on Google.\n",
    "    Retries the request up to 'retries' times in case of failure.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headers = {'User-Agent': get_random_user_agent()}\n",
    "            proxy = get_random_proxy()\n",
    "            search_url = f\"https://www.google.com/search?q={search_query}\"\n",
    "            print(f\"Searching for {search_query}: {search_url}\")\n",
    "            response = requests.get(search_url, headers=headers, proxies=proxy, timeout=10)\n",
    "            \n",
    "            log_request_info(search_query, attempt + 1, proxy)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Using a flexible search for divs with class names that match the pattern\n",
    "                result_elements = soup.find_all('div', class_=re.compile(r'VwiC3b.*yXK7lf.*r025kc.*hJNv6b'))\n",
    "                \n",
    "                for element in result_elements:\n",
    "                    spans = element.find_all('span')\n",
    "                    for span in spans:\n",
    "                        span_text = span.get_text()\n",
    "                        if \"Followers\" in span_text or \"seguidores\" in span_text:\n",
    "                            match = re.search(r'(\\d+(?:,\\d+)*M|\\d+(?:,\\d+)*K|\\d+(?:,\\d+)*)', span_text)\n",
    "                            if match:\n",
    "                                followers = match.group(1)\n",
    "                                followers = followers.replace(',', '')  # Clean the followers count\n",
    "                                return followers\n",
    "                \n",
    "                print(f\"Followers count not found in search results for {search_query}.\")\n",
    "                return None\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Rate limited for {search_query}, status code: {response.status_code}\")\n",
    "                time.sleep(random.uniform(30, 60))  # Longer delay for rate limiting\n",
    "            else:\n",
    "                print(f\"Failed to retrieve search results for {search_query}, status code: {response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for {search_query} on attempt {attempt + 1}: {e}\")\n",
    "            log_request_info(search_query, attempt + 1, proxy, error=str(e))\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(random.uniform(2, 5) * (2 ** attempt))  # Exponential backoff before retrying\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "def get_instagram_followers_from_google(name):\n",
    "    followers = search_followers(f\"{name} site:instagram.com\")\n",
    "    if not followers:\n",
    "        print(f\"Retrying with a different query for {name}\")\n",
    "        followers = search_followers(f\"{name} Instagram\")\n",
    "    return followers\n",
    "\n",
    "def process_names(names, start_index=0):\n",
    "    results = []\n",
    "    for i in range(start_index, len(names)):\n",
    "        name = names[i]\n",
    "        followers = get_instagram_followers_from_google(name)\n",
    "        results.append((name, followers))\n",
    "        time.sleep(random.uniform(10, 30))  # Longer random delay between requests to avoid detection\n",
    "        \n",
    "        # Save progress every 2 names\n",
    "        if (i + 1) % 2 == 0:\n",
    "            temp_df = pd.DataFrame(results, columns=['name', 'Instagram Followers'])\n",
    "            temp_df.to_csv('temp_results.csv', index=False)\n",
    "            print(f\"Progress saved at index {i + 1}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = r'C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/Youtube and Instagram Data/data_final_actores_full_3.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Use the 'name' column for names\n",
    "names = df['name'].tolist()\n",
    "\n",
    "# Select first 3 names for testing\n",
    "names = names[:10]\n",
    "\n",
    "# Check for previous progress\n",
    "if os.path.exists('temp_results.xlsx'):\n",
    "    temp_df = pd.read_excel('temp_results.xlsx')\n",
    "    processed_names = temp_df['name'].tolist()\n",
    "    start_index = len(processed_names)\n",
    "else:\n",
    "    start_index = 0\n",
    "\n",
    "# Process the names to get the Instagram followers\n",
    "followers_list = process_names(names[start_index:], start_index)\n",
    "\n",
    "# Combine with previously processed data if any\n",
    "if start_index > 0:\n",
    "    previous_df = pd.read_excel('temp_results.xlsx')\n",
    "    followers_df = pd.concat([previous_df, pd.DataFrame(followers_list, columns=['name', 'Instagram Followers'])], ignore_index=True)\n",
    "else:\n",
    "    followers_df = pd.DataFrame(followers_list, columns=['name', 'Instagram Followers'])\n",
    "\n",
    "# Merge with original dataframe\n",
    "df = df.merge(followers_df, on='name', how='left')\n",
    "\n",
    "# Save the updated DataFrame back to an Excel file\n",
    "output_file_path = r'C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/Youtube and Instagram Data/data_final_actores_full_3.xlsx'\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"Finished processing and saved to\", output_file_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
