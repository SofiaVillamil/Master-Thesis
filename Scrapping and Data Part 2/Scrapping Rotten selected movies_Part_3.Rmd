---
title: "Scrapping Again Rotten Tomatoes for specific complicated URL's"
author: "Sofia Villamil"
date: "2024-06-05"
output: html_document
---

### SCRAPPING FOR ROTTEN TOMATOES AGAIN

We are going to scrap again for the specific url's found to be NA's in the previous data set. The objective is to attain more data and reduce the missing values for reviews

```{r}
rm(list = ls())
```

```{r}
library(dplyr)
library(stringr)
library(readr)
library(httr)
library(foreach)
library(doParallel)
library(rvest)
library(purrr)
library(furrr)
```

We are using different user agents in order to not compromise the page.

```{r}
user_agents <- c(
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36",
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
  "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
  "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36",
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
  "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15",
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0",
  "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36",
  "Mozilla/5.0 (Macintosh; Intel Mac OS X 11_2_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15",
  "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0"
)

```

This is the data set attained in the previous code for the URL's that were not found.

```{r}
data1 <- read_csv("C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/data_na_titles.csv", show_col_types = FALSE)
```

```{r}
# Subset for testing with a certain amount of URLs
data <- data1[1:10, ]
```

#### Obtaining URL's

We are going to recycle some of the code of the previous scrapping. However, we will change a little the approach by first attaining the URL's, saving them and then scrapping the data.

```{r}
known_variations <- list(
  "The Avengers" = "marvels_the_avengers",
  "Star Wars: The Force Awakens" = "star_wars_episode_vii_the_force_awakens",
  "Once Upon a Time… in Hollywood" = "once_upon_a_time_in_hollywood",
  "Birdman or (The Unexpected Virtue of Ignorance)" = "birdman_2014",
  "Men in Black 3" = "men_in_black_iii",
  "Kung Fu Panda 2" = "kung_fu_panda_the_kaboom_of_doom",
  "Pokémon Detective Pikachu" = "pokemon_detective_pikachu",
  "The Man from U.N.C.L.E." = "the_man_from_uncle",
  "The 5th Wave" = "the_fifth_wave",
  "G.I. Joe: Retaliation" = "gi_joe_retaliation",
  "Les Misérables" = "les_miserables",
  "The Adjustment Bureau" = "adjustment_bureau",
  "Gangster Squad" = "gangster_squad_2012",
  "R.I.P.D." = "ripd",
  "A Silent Voice: The Movie" = "a_silent_voice",
  "The Woman in Black" = "the_woman_in_black_2011",
  "The Wolf of Wall Street" = "the_wolf_of_wall_street_2013",
  "The SpongeBob Movie: Sponge Out of Water" = "the_spongebob_movie_sponge_out_of_water",
  "Your Name." = "your_name_2017"
)

# Function to check if a URL is valid
is_url_valid <- function(url) {
  tryCatch({
    user_agent <- sample(user_agents, 1)
    response <- HEAD(url, user_agent(user_agent))
    response$status_code == 200
  }, error = function(e) {
    FALSE
  })
}

# Function to generate the Rotten Tomatoes URL from this sepecific movies
generate_movie_url <- function(movie_title, release_year = NULL, known_variation = NULL) {
  if (!is.null(known_variation)) {
    formatted_title <- known_variation
  } else {
    movie_title <- str_replace_all(movie_title, "&", "and")
    movie_title <- str_replace_all(movie_title, "[\\$']", "")
    formatted_title <- str_to_lower(str_replace_all(movie_title, "[ :.,!?\"-/]", "_"))
    formatted_title <- str_replace_all(formatted_title, "\\.\\.\\.", "_")
    formatted_title <- str_replace_all(formatted_title, "__+", "_")
    formatted_title <- str_replace_all(formatted_title, "_$", "")
  }
  
  base_url <- paste0("https://www.rottentomatoes.com/m/", formatted_title)
  
  if (is_url_valid(base_url)) {
    return(base_url)
  }
  
  # Check URLs with appended years as some URL's have the year as suffix
  years <- 2010:2024
  possible_urls <- paste0(base_url, "_", years)
  valid_urls <- possible_urls[sapply(possible_urls, is_url_valid)]
  
  if (length(valid_urls) > 0) {
    return(tail(valid_urls, 1))  # Return the URL with the latest year as this are the movies we will use in the analysis.
  }
  
  return(NA)
}

# Function to save intermediate results to a CSV file. To have a backup
save_results <- function(data, file_path) {
  write.csv(data, file_path, row.names = FALSE)
}

# Path to save the intermediate results
file_path <- "C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/generated_urls.csv"

# empty data frame to store results
results <- data.frame(title = character(), generated_url = character(), stringsAsFactors = FALSE)

# Register parallel backend
num_cores <- detectCores() - 1  # Use one less core than available, I have 8 so i use 7. But this function checks it!
cl <- makeCluster(num_cores)
registerDoParallel(cl)

# Parallel loop through each title, generate URL, and save results incrementally to have a back up
results <- foreach(i = 1:nrow(data), .combine = rbind, .packages = c("dplyr", "stringr", "httr")) %dopar% {
  title <- data$title[i]
  known_variation <- known_variations[[title]]
  generated_url <- generate_movie_url(title, known_variation = known_variation)
  
  data.frame(title = title, generated_url = generated_url, stringsAsFactors = FALSE)
}


# Save the final results
save_results(results, file_path)

# Stop the cluster
stopCluster(cl)
```

Some Movies where not found in the page so I will eliminate them from the data set to have less movies to search.

```{r}
results2 <- results[!is.na(results$generated_url), ]
save_results(results2, file_path)
```

#### Scrapping the data

```{r}
# Function to extract single text 
extract_single_text <- function(node) {
  if (length(node) == 0) return(NA)
  html_text(node, trim = TRUE)
}

# Function to scrape movie data from a given URL
scrape_movie_data <- function(url) {
  Sys.sleep(sample(5:10, 1))

  movie_page <- read_html(url)
  
  title_rt <- extract_single_text(movie_page %>% html_node(xpath = '//h1[@slot="titleIntro"]//span'))
  critics_score_rt <- movie_page %>%
    html_node(xpath = '//rt-button[@slot="criticsScore"]//rt-text') %>%
    html_text(trim = TRUE) %>%
    gsub("%", "", .) %>%
    as.numeric()

  total_critics_reviews_rt <- movie_page %>%
    html_node(xpath = '//rt-link[@slot="criticsReviews" and @size="0.75"]') %>%
    html_text(trim = TRUE) %>%
    gsub("\\s+Reviews", "", .) %>%
    gsub(",", "", .) %>%
    as.numeric()

  audience_score_rt <- movie_page %>%
    html_node(xpath = '//rt-button[@slot="audienceScore"]//rt-text') %>%
    html_text(trim = TRUE) %>%
    gsub("%", "", .) %>%
    as.numeric()
  
  total_audience_reviews_rt <- movie_page %>%
    html_node(xpath = '//rt-link[@slot="audienceReviews" and @size="0.75"]') %>%
    html_text(trim = TRUE) %>%
    gsub("[^0-9]", "", .) %>%
    as.numeric()
  
  top_critic_review_1_rt <- extract_single_text(movie_page %>% 
    html_node(xpath = '(//carousel-slider//review-card-critic[@slot="tile"])[1]//rt-text[@slot="content"]'))

  top_critic_review_2_rt <- extract_single_text(movie_page %>% 
    html_node(xpath = '(//carousel-slider//review-card-critic[@slot="tile"])[2]//rt-text[@slot="content"]'))
  
  audience_review_1_rt <- extract_single_text(movie_page %>% 
    html_node(xpath = '(//carousel-slider//review-card-audience[@slot="tile"])[1]//rt-text[@slot="content"]'))
  
  audience_review_2_rt <- extract_single_text(movie_page %>% 
    html_node(xpath = '(//carousel-slider//review-card-audience[@slot="tile"])[2]//rt-text[@slot="content"]'))

  data.frame(
    title_rt = title_rt,
    Title = title_rt,
    critics_score_rt = critics_score_rt,
    total_critics_reviews_rt = total_critics_reviews_rt,
    audience_score_rt = audience_score_rt,
    total_audience_reviews_rt = total_audience_reviews_rt,
    top_critic_review_1_rt = top_critic_review_1_rt,
    top_critic_review_2_rt = top_critic_review_2_rt,
    audience_review_1_rt = audience_review_1_rt,
    audience_review_2_rt = audience_review_2_rt,
    stringsAsFactors = FALSE
  )
}

# Select the number URL's we want to scrap
test <- results2[1:10, ]

# Parallel processing setup to make the procces faster
plan(multisession, workers = 7)  # Adjust the number of workers as needed

# Scraping all URLs
movies_data <- test %>%
  mutate(movie_data = future_map(generated_url, safely(scrape_movie_data), .options = furrr_options(seed = TRUE))) %>%
  mutate(movie_data = map(movie_data, "result"))

# Combining all
combined_data <- bind_rows(movies_data$movie_data)

# Save the combined data to a CSV file to have a back up
write.csv(combined_data, "C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/scraped_movie_data.csv", row.names = FALSE)
```

### Joining the data attained with the final data set

```{r}
data_final <- read_csv("C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/data_final.csv",show_col_types = FALSE)
```

The titles in Rotten Tomatoes do not mach the ones in data final. So we fix it by doing this as the order was not disturbed.

```{r}
combined_data$title_generated_urls <- results2$title
```

I checked and every title seems to be corrected, so we can merge now.

```{r}
merged_data <- left_join(data_final, combined_data, by = c("title" = "title_generated_urls"))

```

**Fixing the extra columns generated by the merge**

```{r}
merged_data <- merged_data %>%
  mutate(
    critics_score_rt = coalesce(critics_score_rt.x, critics_score_rt.y),
    total_critics_reviews_rt = coalesce(total_critics_reviews_rt.x, total_critics_reviews_rt.y),
    audience_score_rt = coalesce(audience_score_rt.x, audience_score_rt.y),
    total_audience_reviews_rt = coalesce(total_audience_reviews_rt.x, total_audience_reviews_rt.y),
    top_critic_review_1_rt = coalesce(top_critic_review_1_rt.x, top_critic_review_1_rt.y),
    top_critic_review_2_rt = coalesce(top_critic_review_2_rt.x, top_critic_review_2_rt.y),
    audience_review_1_rt = coalesce(audience_review_1_rt.x, audience_review_1_rt.y),
    audience_review_2_rt = coalesce(audience_review_2_rt.x, audience_review_2_rt.y)
  ) %>%
  select(-ends_with(".x"), -ends_with(".y"))


```

```{r}
write.csv(merged_data, "C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/data_final_3.csv",row.names = FALSE)

```
