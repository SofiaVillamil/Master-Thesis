---
title: "Scrapping for BoxMojo"
author: "Sofia Villamil"
date: "2024-06-02"
output: html_document
---

### BOX MOJO SCRAPPING

We are going to scrap the website BoxMojo (Link: <https://www.boxofficemojo.com/>) in order to complement the data set with more variables that we could not found in the previous website.

```{r}
rm(list = ls())
```

#### Libraries

```{r}
library(rvest)
library(dplyr)
library(readr)
library(future.apply) # parallel processing
```

```{r}
set_config(user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36; Sofia Villamil / sofia.v1999@gmail.com")) # set user agent here

```

### Scrapping function

```{r}
scrape_boxofficemojo <- function(imdb_id, index) {
  cat(sprintf("Scraping Box Office Mojo data for ID: %s (Observation %d)\n", imdb_id, index))
  
  url <- paste0("https://www.boxofficemojo.com/title/", imdb_id)
  movie_page <- tryCatch({
    read_html(url)
  }, error = function(e) {
    cat("Error in fetching URL:", url, "\n")
    return(NULL)
  }) 
  
  if (is.null(movie_page)) return(NULL)
  
  extract_single_text <- function(node) {
    text <- html_text(node, trim = TRUE)
    if (length(text) > 0) text else NA
  }
  
  # Title
  title_node <- movie_page %>% html_node(xpath = '//h1[contains(@class, "a-size-extra-large")]')
  title <- title_node %>% html_text() %>% trimws() %>% sub("\\s*\\(.*?\\)$", "", .)
  
  # Domestic Distributor
  domestic_distributor <- movie_page %>% 
    html_node(xpath = '//div[span[text()="Domestic Distributor"]]/span[2]') %>%
    extract_single_text()
  
  # Budget
  budget <- movie_page %>% 
    html_node(xpath = '//div[span[text()="Budget"]]/span[2]/span') %>%
    extract_single_text()
  
  # Domestic Opening
  domestic_opening <- movie_page %>% 
    html_node(xpath = '//div[span[text()="Domestic Opening"]]/span[2]/a/span') %>%
    extract_single_text()

 # Domestic Gross
  domestic_gross <- movie_page %>% 
    html_node(xpath = '//div[span[contains(text(), "Domestic")]]/span[contains(@class, "a-size-medium")]/span[contains(@class, "money")]') %>%
    extract_single_text()
  
  # International Gross
  international_gross <- movie_page %>% 
    html_node(xpath = '//div[span[contains(text(), "International")]]/span[contains(@class, "a-size-medium")]/span[contains(@class, "money")]') %>%
    extract_single_text()

  # Worldwide Gross
  worldwide_gross <- movie_page %>% 
    html_node(xpath = '//div[span[contains(text(), "Worldwide")]]/span[contains(@class, "a-size-medium")]/span[contains(@class, "money")]') %>%
    extract_single_text()

  # Number of Markets Released
  number_of_markets <- movie_page %>%
    html_node(xpath = '//tr[contains(@class, "mojo-revenue-source-theatrical")]//td[3]/a') %>%
    extract_single_text()
  number_of_markets <- sub(" markets$", "", number_of_markets)  
  
  Sys.sleep(2)

  data.frame(
    imdb_id = imdb_id, Title = title, 
    DomesticDistributor = domestic_distributor, Budget = budget,
    DomesticOpening = domestic_opening, DomesticGross = domestic_gross,
    InternationalGross = international_gross, WorldwideGross = worldwide_gross,
    NumberOfMarkets = number_of_markets,
    stringsAsFactors = FALSE
  )
}
```

```{r}
# Load data
movies <- read_csv("C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/movies_final_dataset_test.csv",show_col_types = FALSE)

# Example data set to try small number of movies then change to the data
test_movies <- movies

scraped_data <- data.frame()

# Use parallel processing for scraping
plan(multisession, workers = 7) # Adjust the number of workers based on your CPU, I have 8 so I put one less

# Scrape data for each movie in the test data set
scraped_data_list <- future_lapply(seq_len(nrow(test_movies)), function(i) {
  scrape_boxofficemojo(test_movies$imdb_id[i], i)
})

# Combine the results into a single data frame
scraped_data <- bind_rows(scraped_data_list)

# Suffix to be added to each column name for reference as i have many variables from dif places
suffix <- "_bm"
 
# Appending the suffix, except the first column for organization
scraped_data <- scraped_data %>%
  rename_with(~ paste0(., suffix), -1) 

```

```{r}
# Merge
final_data <- left_join(test_movies, scraped_data, by = "imdb_id")

# Saving data set
write.csv(final_data, "C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/movies_final_dataset_test_with_bm.csv", row.names = FALSE)
```

```{r}
# checking columns for NA's
columns_to_check <- c("DomesticDistributor_bm", "Budget_bm", 
                      "DomesticOpening_bm", "DomesticGross_bm", 
                      "InternationalGross_bm", "WorldwideGross_bm", "NumberOfMarkets_bm")

# Drop rows where all specified columns are NA because they are not found in the site
df_cleaned <- final_data %>%
  filter(!(rowSums(is.na(select(., all_of(columns_to_check)))) == length(columns_to_check)))

```

```{r}
write.csv(df_cleaned, "C:/Users/sofia/Documents/Documentos/Master Computational Social Sciences/TFM/Master Thesis/Scrapping and Data Part 2/movies_final_dataset_test_with_bm_cleaned.csv", row.names = FALSE)
```

We are now going to continue to the next script called "Rotten Tomatoes variables scrapping_Part_4"
